{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "meme prediction using pytrends\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "import random\n",
        "import pickle\n",
        "from datetime import datetime, timedelta\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pytrends.request import TrendReq\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error, r2_score\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MemeBenchmarker:\n",
        "    def __init__(self):\n",
        "        self.pytrends = TrendReq(hl='en-US', tz=360)\n",
        "        self.models = {}\n",
        "        self.feature_names = []\n",
        "        self.meme_data = []\n",
        "        \n",
        "        self.models = {\n",
        "            'lifespan': RandomForestRegressor(n_estimators=100, random_state=42),\n",
        "            'peak_timing': GradientBoostingRegressor(random_state=42),\n",
        "            'decay_rate': LinearRegression(),\n",
        "            'viral_potential': GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, random_state=42),\n",
        "            'sustainability': RandomForestRegressor(n_estimators=50, random_state=42)\n",
        "        }\n",
        "        \n",
        "        print(\"Meme Benchmarker initialized with 5 specialized models!\")\n",
        "    \n",
        "    def get_trends_data(self, meme_name, timeframe='2022-01-01 2024-12-31'):\n",
        "        try:\n",
        "            print(f\"Fetching trends for: {meme_name}\")\n",
        "            \n",
        "            self.pytrends.build_payload([meme_name], cat=0, timeframe=timeframe, geo='', gprop='')\n",
        "            interest_data = self.pytrends.interest_over_time()\n",
        "            \n",
        "            if interest_data.empty:\n",
        "                print(f\"No data found for {meme_name}\")\n",
        "                return None\n",
        "                \n",
        "            print(f\"Found {len(interest_data)} data points for {meme_name}\")\n",
        "            return interest_data\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"Error getting trends for {meme_name}: {str(e)}\")\n",
        "            return None\n",
        "    \n",
        "    def extract_comprehensive_features(self, trends_df, meme_name):\n",
        "        if trends_df is None or trends_df.empty:\n",
        "            return None\n",
        "            \n",
        "        interest_values = trends_df[meme_name].values\n",
        "        \n",
        "        features = {\n",
        "            'meme_name': meme_name,\n",
        "            'max_interest': np.max(interest_values),\n",
        "            'mean_interest': np.mean(interest_values),\n",
        "            'median_interest': np.median(interest_values),\n",
        "            'std_interest': np.std(interest_values),\n",
        "            'variance': np.var(interest_values),\n",
        "            'total_points': len(interest_values),\n",
        "            'non_zero_points': np.count_nonzero(interest_values),\n",
        "            'zero_ratio': np.sum(interest_values == 0) / len(interest_values)\n",
        "        }\n",
        "        \n",
        "        peak_idx = np.argmax(interest_values)\n",
        "        features['peak_position'] = peak_idx / len(interest_values)\n",
        "        features['peak_value'] = interest_values[peak_idx]\n",
        "        \n",
        "        if peak_idx < len(interest_values) - 1:\n",
        "            post_peak = interest_values[peak_idx:]\n",
        "            features['decay_rate'] = self._calculate_decay_rate(post_peak)\n",
        "            features['sustained_interest'] = np.mean(post_peak)\n",
        "        else:\n",
        "            features['decay_rate'] = 0\n",
        "            features['sustained_interest'] = 0\n",
        "        \n",
        "        if peak_idx > 0:\n",
        "            pre_peak = interest_values[:peak_idx]\n",
        "            features['growth_rate'] = self._calculate_growth_rate(pre_peak)\n",
        "        else:\n",
        "            features['growth_rate'] = 0\n",
        "        \n",
        "        features['skewness'] = self._calculate_skewness(interest_values)\n",
        "        features['kurtosis'] = self._calculate_kurtosis(interest_values)\n",
        "        \n",
        "        features['volatility'] = np.std(np.diff(interest_values))\n",
        "        features['max_change'] = np.max(np.abs(np.diff(interest_values)))\n",
        "        \n",
        "        features['trend_persistence'] = self._calculate_trend_persistence(interest_values)\n",
        "        \n",
        "        features['lifespan_days'] = self._calculate_lifespan(interest_values)\n",
        "        features['peak_timing_ratio'] = features['peak_position']\n",
        "        features['decay_rate_target'] = features['decay_rate']\n",
        "        features['viral_score'] = self._calculate_viral_score(interest_values)\n",
        "        features['sustainability_score'] = self._calculate_sustainability_score(interest_values)\n",
        "        \n",
        "        return features\n",
        "    \n",
        "    def _calculate_decay_rate(self, values):\n",
        "        if len(values) < 2:\n",
        "            return 0\n",
        "        x = np.arange(len(values))\n",
        "        if np.sum(x**2) == 0:\n",
        "            return 0\n",
        "        slope = np.polyfit(x, values, 1)[0]\n",
        "        return abs(slope)\n",
        "    \n",
        "    def _calculate_growth_rate(self, values):\n",
        "        if len(values) < 2:\n",
        "            return 0\n",
        "        x = np.arange(len(values))\n",
        "        if np.sum(x**2) == 0:\n",
        "            return 0\n",
        "        slope = np.polyfit(x, values, 1)[0]\n",
        "        return slope\n",
        "    \n",
        "    def _calculate_skewness(self, values):\n",
        "        if len(values) < 3:\n",
        "            return 0\n",
        "        mean_val = np.mean(values)\n",
        "        std_val = np.std(values)\n",
        "        if std_val == 0:\n",
        "            return 0\n",
        "        return np.mean(((values - mean_val) / std_val) ** 3)\n",
        "    \n",
        "    def _calculate_kurtosis(self, values):\n",
        "        if len(values) < 4:\n",
        "            return 0\n",
        "        mean_val = np.mean(values)\n",
        "        std_val = np.std(values)\n",
        "        if std_val == 0:\n",
        "            return 0\n",
        "        return np.mean(((values - mean_val) / std_val) ** 4) - 3\n",
        "    \n",
        "    def _calculate_trend_persistence(self, values):\n",
        "        if len(values) < 2:\n",
        "            return 0\n",
        "        changes = np.diff(values)\n",
        "        positive_changes = np.sum(changes > 0)\n",
        "        total_changes = len(changes)\n",
        "        if total_changes == 0:\n",
        "            return 0\n",
        "        return positive_changes / total_changes\n",
        "    \n",
        "    def _calculate_lifespan(self, values):\n",
        "        max_interest = np.max(values)\n",
        "        peak_idx = np.argmax(values)\n",
        "        \n",
        "        rise_threshold = max_interest * 0.2\n",
        "        first_active = peak_idx\n",
        "        for i in range(peak_idx + 1):\n",
        "            if values[i] >= rise_threshold:\n",
        "                first_active = i\n",
        "                break\n",
        "        \n",
        "        death_threshold = max_interest * 0.25\n",
        "        consecutive_low = 0\n",
        "        last_active = peak_idx\n",
        "        \n",
        "        for i in range(peak_idx, len(values)):\n",
        "            if values[i] >= death_threshold:\n",
        "                last_active = i\n",
        "                consecutive_low = 0\n",
        "            else:\n",
        "                consecutive_low += 1\n",
        "                if consecutive_low >= 6:\n",
        "                    last_active = i - 6\n",
        "                    break\n",
        "                last_active = i\n",
        "        \n",
        "        span_weeks = max(1, last_active - first_active + 1)\n",
        "        span_days = span_weeks * 7\n",
        "        \n",
        "        return span_days\n",
        "    \n",
        "    def _calculate_viral_score(self, values):\n",
        "        max_interest = np.max(values)\n",
        "        mean_interest = np.mean(values)\n",
        "        \n",
        "        peak_idx = np.argmax(values)\n",
        "        peak_position = peak_idx / len(values) if len(values) > 0 else 0.5\n",
        "        \n",
        "        if peak_idx > 0:\n",
        "            pre_peak = values[:peak_idx]\n",
        "            if len(pre_peak) > 1:\n",
        "                growth_rate = np.mean(np.diff(pre_peak))\n",
        "            else:\n",
        "                growth_rate = 0\n",
        "        else:\n",
        "            growth_rate = 0\n",
        "        \n",
        "        if peak_idx > 0:\n",
        "            velocity = max_interest / (peak_idx + 1)\n",
        "        else:\n",
        "            velocity = max_interest\n",
        "        \n",
        "        top_quartile_threshold = np.percentile(values, 75)\n",
        "        momentum = np.sum(values > top_quartile_threshold) / len(values) if len(values) > 0 else 0\n",
        "        \n",
        "        max_interest_norm = max_interest / 100.0\n",
        "        peak_early_factor = max(0, 1 - peak_position * 2)\n",
        "        growth_factor = max(0, min(1, (growth_rate + 5) / 10))\n",
        "        velocity_factor = min(1, velocity / 10)\n",
        "        momentum_factor = momentum\n",
        "        \n",
        "        viral_score = (\n",
        "            0.3 * max_interest_norm +\n",
        "            0.25 * peak_early_factor +\n",
        "            0.2 * growth_factor +\n",
        "            0.15 * velocity_factor +\n",
        "            0.1 * momentum_factor\n",
        "        )\n",
        "        \n",
        "        return min(max(viral_score, 0.0), 1.0)\n",
        "    \n",
        "    def _calculate_sustainability_score(self, values):\n",
        "        mean_interest = np.mean(values)\n",
        "        sustained_interest = np.mean(values[len(values)//2:])\n",
        "        decay_rate = self._calculate_decay_rate(values)\n",
        "        \n",
        "        sustainability = (mean_interest / 100) * (sustained_interest / 100) * (1 - decay_rate)\n",
        "        return min(sustainability, 1.0)\n",
        "    \n",
        "    def create_meme_dataset(self, meme_list=None):\n",
        "        if meme_list is None:\n",
        "            meme_list = [\n",
        "                \"distracted boyfriend\", \"woman yelling at cat\", \"this is fine\", \"drake pointing\",\n",
        "                \"change my mind\", \"expanding brain\", \"galaxy brain\", \"stonks\", \"big chungus\",\n",
        "                \"wojak\", \"pepe\", \"doge\", \"crying laughing emoji\", \"skull emoji\", \"capybara\",\n",
        "                \"sigma grindset\", \"main character\", \"NPC\", \"chad\", \"virgin\", \"gigachad\",\n",
        "                \"soyjak\", \"doomer\", \"boomer\", \"zoomer\", \"ok boomer\", \"no cap\", \"bet\",\n",
        "                \"fr fr\", \"on god\", \"bussin\", \"slay\", \"periodt\", \"and I oop\", \"sksksk\",\n",
        "                \"vsco girl\", \"e-girl\", \"simps\", \"thirst trap\", \"main character energy\",\n",
        "                \"pick me girl\", \"not like other girls\", \"basic\", \"extra\", \"cringe\",\n",
        "                \"based\", \"redpilled\", \"bluepilled\", \"woke\", \"cancel culture\", \"Karen\",\n",
        "                \"millennial\", \"gen z\", \"gen alpha\", \"cheugy\", \"stan\", \"ship\", \"OTP\",\n",
        "                \"BFF\", \"FOMO\", \"YOLO\", \"main character syndrome\", \"pick me energy\",\n",
        "                \"hot girl summer\", \"side character\", \"background character\", \"NPC energy\",\n",
        "                \"protagonist energy\", \"antagonist energy\", \"villain arc\", \"redemption arc\",\n",
        "                \"character development\", \"plot twist\", \"cliffhanger\", \"season finale\",\n",
        "                \"series finale\", \"spin-off\", \"reboot\", \"remake\", \"sequel\", \"prequel\",\n",
        "                \"rizz\", \"skibidi toilet\", \"ohio\", \"fanum tax\", \"sigma\", \"alpha\",\n",
        "                \"beta\", \"gamma\", \"delta\", \"epsilon\", \"zeta\", \"eta\", \"theta\",\n",
        "                \"iota\", \"kappa\", \"lambda\", \"mu\", \"nu\", \"xi\", \"omicron\", \"pi\",\n",
        "                \"rho\", \"tau\", \"upsilon\", \"phi\", \"chi\", \"psi\", \"omega\"\n",
        "            ]\n",
        "        \n",
        "        print(f\"Creating comprehensive meme dataset with {len(meme_list)} memes...\")\n",
        "        \n",
        "        for i, meme in enumerate(meme_list):\n",
        "            print(f\"Processing {i+1}/{len(meme_list)}: {meme}\")\n",
        "            \n",
        "            trends_data = self.get_trends_data(meme)\n",
        "            \n",
        "            if trends_data is not None:\n",
        "                features = self.extract_comprehensive_features(trends_data, meme)\n",
        "                \n",
        "                if features:\n",
        "                    self.meme_data.append(features)\n",
        "                    print(f\"{meme}: {features['lifespan_days']} days, viral: {features['viral_score']:.2f}\")\n",
        "                else:\n",
        "                    print(f\"{meme}: Could not extract features\")\n",
        "            else:\n",
        "                print(f\"{meme}: No trends data found\")\n",
        "            \n",
        "            time.sleep(random.uniform(2, 4))\n",
        "        \n",
        "        print(f\"Dataset created with {len(self.meme_data)} memes!\")\n",
        "        return self.meme_data\n",
        "    \n",
        "    def train_ensemble_models(self, verbose=True):\n",
        "        if len(self.meme_data) < 10:\n",
        "            if verbose:\n",
        "                print(\"Not enough data to train models!\")\n",
        "            return False\n",
        "        \n",
        "        if verbose:\n",
        "            print(\"Training ensemble of specialized models...\")\n",
        "        \n",
        "        df = pd.DataFrame(self.meme_data)\n",
        "        \n",
        "        exclude_cols = ['meme_name', 'lifespan_days', 'peak_timing_ratio', \n",
        "                       'decay_rate_target', 'viral_score', 'sustainability_score']\n",
        "        self.feature_names = [col for col in df.columns if col not in exclude_cols]\n",
        "        \n",
        "        X = df[self.feature_names]\n",
        "        \n",
        "        model_results = {}\n",
        "        \n",
        "        for model_name, model in self.models.items():\n",
        "            if verbose:\n",
        "                print(f\"Training {model_name} model...\")\n",
        "            \n",
        "            if model_name == 'lifespan':\n",
        "                y = df['lifespan_days']\n",
        "            elif model_name == 'peak_timing':\n",
        "                y = df['peak_timing_ratio']\n",
        "            elif model_name == 'decay_rate':\n",
        "                y = df['decay_rate_target']\n",
        "            elif model_name == 'viral_potential':\n",
        "                y = df['viral_score']\n",
        "            elif model_name == 'sustainability':\n",
        "                y = df['sustainability_score']\n",
        "            \n",
        "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "            \n",
        "            if model_name == 'viral_potential':\n",
        "                mask_train = np.isfinite(y_train.values)\n",
        "                mask_test = np.isfinite(y_test.values)\n",
        "                X_train = X_train[mask_train]\n",
        "                y_train = y_train[mask_train]\n",
        "                X_test = X_test[mask_test]\n",
        "                y_test = y_test[mask_test]\n",
        "            \n",
        "            try:\n",
        "                model.fit(X_train, y_train)\n",
        "                \n",
        "                y_pred = model.predict(X_test)\n",
        "                if model_name == 'viral_potential':\n",
        "                    y_pred = np.clip(y_pred, 0, 1)\n",
        "                \n",
        "                mae = mean_absolute_error(y_test, y_pred)\n",
        "                r2 = r2_score(y_test, y_pred)\n",
        "                \n",
        "                model_results[model_name] = {'mae': mae, 'r2': r2}\n",
        "                if verbose:\n",
        "                    print(f\"   {model_name}: MAE={mae:.3f}, RÂ²={r2:.3f}\")\n",
        "            except Exception as e:\n",
        "                if verbose:\n",
        "                    print(f\"   {model_name}: Error during training - {str(e)}\")\n",
        "                model_results[model_name] = {'mae': float('inf'), 'r2': -999}\n",
        "        \n",
        "        if verbose:\n",
        "            print(\"All models trained successfully!\")\n",
        "        return True\n",
        "    \n",
        "    def predict_meme_performance(self, meme_name):\n",
        "        if not all(model is not None for model in self.models.values()):\n",
        "            print(\"Models not trained yet!\")\n",
        "            return None\n",
        "        \n",
        "        print(f\"Predicting performance for: {meme_name}\")\n",
        "        \n",
        "        trends_data = self.get_trends_data(meme_name)\n",
        "        \n",
        "        if trends_data is None:\n",
        "            print(f\"Could not get trends data for {meme_name}\")\n",
        "            return None\n",
        "        \n",
        "        features = self.extract_comprehensive_features(trends_data, meme_name)\n",
        "        \n",
        "        if features is None:\n",
        "            print(f\"Could not extract features for {meme_name}\")\n",
        "            return None\n",
        "        \n",
        "        feature_values = [features[col] for col in self.feature_names]\n",
        "        X = np.array(feature_values).reshape(1, -1)\n",
        "        \n",
        "        predictions = {}\n",
        "        for model_name, model in self.models.items():\n",
        "            pred = model.predict(X)[0]\n",
        "            if model_name == 'viral_potential':\n",
        "                pred = np.clip(pred, 0.0, 1.0)\n",
        "            predictions[model_name] = pred\n",
        "        \n",
        "        result = {\n",
        "            'meme_name': meme_name,\n",
        "            'predictions': predictions,\n",
        "            'features': features,\n",
        "            'trends_data': trends_data,\n",
        "            'summary': self._create_prediction_summary(predictions)\n",
        "        }\n",
        "        \n",
        "        return result\n",
        "    \n",
        "    def _create_prediction_summary(self, predictions):\n",
        "        lifespan = predictions['lifespan']\n",
        "        viral = np.clip(predictions['viral_potential'], 0.0, 1.0)\n",
        "        sustainability = np.clip(predictions['sustainability'], 0.0, 1.0)\n",
        "        peak_timing = np.clip(predictions['peak_timing'], 0.0, 1.0)\n",
        "        decay = max(0, predictions['decay_rate'])\n",
        "        \n",
        "        summary = {\n",
        "            'lifespan_days': round(lifespan, 1),\n",
        "            'lifespan_weeks': round(lifespan / 7, 1),\n",
        "            'viral_potential': f\"{viral:.1%}\",\n",
        "            'sustainability': f\"{sustainability:.1%}\",\n",
        "            'peak_timing': f\"{peak_timing:.1%} through timeline\",\n",
        "            'decay_rate': f\"{decay:.3f}\",\n",
        "            'overall_score': round((viral + sustainability) / 2, 2)\n",
        "        }\n",
        "        \n",
        "        return summary\n",
        "    \n",
        "    def compare_memes(self, meme1, meme2):\n",
        "        print(f\"Comparing: {meme1} vs {meme2}\")\n",
        "        \n",
        "        pred1 = self.predict_meme_performance(meme1)\n",
        "        pred2 = self.predict_meme_performance(meme2)\n",
        "        \n",
        "        if pred1 is None or pred2 is None:\n",
        "            print(\"Could not get predictions for one or both memes\")\n",
        "            return None\n",
        "        \n",
        "        comparison = {\n",
        "            'meme1': {'name': meme1, 'predictions': pred1['predictions'], 'summary': pred1['summary']},\n",
        "            'meme2': {'name': meme2, 'predictions': pred2['predictions'], 'summary': pred2['summary']},\n",
        "            'winner': self._determine_winner(pred1['predictions'], pred2['predictions'])\n",
        "        }\n",
        "        \n",
        "        return comparison\n",
        "    \n",
        "    def _determine_winner(self, pred1, pred2):\n",
        "        categories = {\n",
        "            'lifespan': 'Longer lasting',\n",
        "            'viral_potential': 'More viral',\n",
        "            'sustainability': 'More sustainable',\n",
        "            'peak_timing': 'Peaks earlier' if pred1['peak_timing'] < pred2['peak_timing'] else 'Peaks later',\n",
        "            'decay_rate': 'Decays slower' if pred1['decay_rate'] < pred2['decay_rate'] else 'Decays faster'\n",
        "        }\n",
        "        \n",
        "        winners = {}\n",
        "        for category, description in categories.items():\n",
        "            if pred1[category] > pred2[category]:\n",
        "                winners[category] = 'meme1'\n",
        "            else:\n",
        "                winners[category] = 'meme2'\n",
        "        \n",
        "        overall1 = (pred1['viral_potential'] + pred1['sustainability']) / 2\n",
        "        overall2 = (pred2['viral_potential'] + pred2['sustainability']) / 2\n",
        "        \n",
        "        winners['overall'] = 'meme1' if overall1 > overall2 else 'meme2'\n",
        "        \n",
        "        return winners\n",
        "    \n",
        "    def save_models(self, filename='meme_benchmarker_models.pkl'):\n",
        "        model_data = {\n",
        "            'models': self.models,\n",
        "            'feature_names': self.feature_names,\n",
        "            'meme_data': self.meme_data\n",
        "        }\n",
        "        \n",
        "        with open(filename, 'wb') as f:\n",
        "            pickle.dump(model_data, f)\n",
        "        \n",
        "        print(f\"Models saved to {filename}\")\n",
        "        return True\n",
        "    \n",
        "    def load_models(self, filename='meme_benchmarker_models.pkl'):\n",
        "        try:\n",
        "            with open(filename, 'rb') as f:\n",
        "                model_data = pickle.load(f)\n",
        "            \n",
        "            self.models = model_data['models']\n",
        "            self.feature_names = model_data['feature_names']\n",
        "            self.meme_data = model_data['meme_data']\n",
        "            \n",
        "            print(f\"Models loaded from {filename}\")\n",
        "            return True\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"Error loading models: {str(e)}\")\n",
        "            return False\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "benchmarker = MemeBenchmarker()\n",
        "\n",
        "if not benchmarker.load_models():\n",
        "    print(\"No existing models found, creating new ones...\")\n",
        "    benchmarker.create_meme_dataset()\n",
        "    if benchmarker.train_ensemble_models():\n",
        "        benchmarker.save_models()\n",
        "    else:\n",
        "        print(\"Failed to train models!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "meme_name = \"doge\"\n",
        "result = benchmarker.predict_meme_performance(meme_name)\n",
        "\n",
        "if result:\n",
        "    print(f\"\\nPerformance Prediction for '{meme_name}':\")\n",
        "    for key, value in result['summary'].items():\n",
        "        print(f\"   {key}: {value}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "meme1 = \"doge\"\n",
        "meme2 = \"pepe\"\n",
        "\n",
        "comparison = benchmarker.compare_memes(meme1, meme2)\n",
        "\n",
        "if comparison:\n",
        "    print(f\"\\n{meme1} vs {meme2} Results:\")\n",
        "    print(f\"Overall Winner: {comparison['winner']['overall']}\")\n",
        "    for category, winner in comparison['winner'].items():\n",
        "        if category != 'overall':\n",
        "            print(f\"{category}: {winner}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Model Information:\")\n",
        "print(f\"Number of models: {len(benchmarker.models)}\")\n",
        "print(f\"Features: {len(benchmarker.feature_names)}\")\n",
        "print(f\"Training samples: {len(benchmarker.meme_data)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
